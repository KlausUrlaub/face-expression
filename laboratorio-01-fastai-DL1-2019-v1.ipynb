{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "# Laboratório 01 - fast.ai DL1 2019 Brasília\n",
    "\n",
    "\n",
    "### Introdução\n",
    "\n",
    "Bem vindos ao primeiro laboratório da turma fast.ai DL1 2019. O objetivo do laboratório é consolidar os conhecimentos obtidos nas aulas 01 e 02, bem como promover debate, discussões relacionadas ao tema, bem como fazer com que a maioria coloque a \"mão na massa\" como gosta nosso guru, Jeremy.\n",
    "\n",
    "Realizaremos nesse laboratório o treinamento de uma rede neural para classificação de expressões faciais baseadas em fotos de pessoas. Utilizaremos a base de dados de um desafio do Kaggle de 2013, conforme detalhes mais adiante. O modelo treinado terá alcançado uma acurácia similiar, ou até superior, aos melhores modelos da competição em 2013. Além disso, vamos desenvolver um aplicativo web simples para colocar em produção nosso modelo treinado de a nos permitir testar novas imagens com expressões a avaliar a acurácia da rede neural na prática.\n",
    "\n",
    "Esse notebook é uma combinação de dois posts do colega [Pierre Guillou](https://www.linkedin.com/in/pierreguillou/), que cedeu seu código para que possamos consolidar nossos conhecimentos:\n",
    "\n",
    "- [Face Expression Recognition with fastai v1](https://medium.com/@pierre_guillou/face-expression-recognition-with-fastai-v1-dc4cf6b141a3).\n",
    "- [Deep Learning Web App by fastai v1](https://medium.com/@pierre_guillou/deep-learning-web-app-by-fastai-v1-3ab4c20b7cac)\n",
    "\n",
    "\n",
    "### Reconhecimento de Expressões Faciais (Face Expression Recognition Kaggle Challenge - FER 2013) com fastai v1\n",
    "\n",
    "by: [Pierre Guillou](https://www.linkedin.com/in/pierreguillou/) (novembro 2018)  \n",
    "    [Leon Silva](https://www.linkedin.com/in/leonsolon/) (tradução/atualização março 2019)\n",
    "    [Vinícius Ramos](https://www.linkedin.com/in/vinicius-ramos-6a367073/) (atualização/deploy no Django na GCP março 2018)\n",
    "\n",
    "- Inspirado por [Recognizing Facial Expressions Using Deep Learning](http://cs231n.stanford.edu/reports/2017/pdfs/224.pdf) (2017) of Alexandru Savoiu and James Wong (Stanford University)\n",
    "- Base de dados : [FER 2013 Kaggle Challenge](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/)\n",
    "- Deep Learning utilizando [fastai v1](https://www.fast.ai/2018/10/02/fastai-ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicialização\n",
    "\n",
    "Vamos começar como todo e qualquer notebook que utiliza fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuração da API Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faremos o download do banco de imagens diretamente do Kaggle. Para tal, é necessário seguir os passos abaixo. Este notebook está focalizado na google cloud platform (GCP), tendo em vista que a maioria da turma realizou essa instalação. Se você está lendo isso é porque já está rodando o jupyter de alguma forma, seja na máquina local, seja no gcp, portanto, pularemos os passos de inicialização da instância e do jupyter.\n",
    "\n",
    "1. Caso ainda não possua, cria sua conta no [Kaggle](https://www.kaggle.com) no seu computador local\n",
    "2. No canto superior direito, clique na sua foto (ou avatar) e selecione 'My account'\n",
    "3. Selecione a opção 'Create API Token'. \n",
    "4. Ao selecionar a opção será baixado um arquivo na sua máquina, chamada `kaggle.json`. Esse arquivo contém as credenciais da API do Kaggle (nome de usuário e senha)\n",
    "5. Abra o arquivo `kaggle.json` no seu editor de textos preferido e copie o conteúdo. Cole o conteúdo na célula abaixo, no local indicado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comando que cria a pasta Kaggle em /home/<seu_usuario>. Essa pasta é necessária para conter a chave de API do Kaggle\n",
    "!mkdir ~/.kaggle\n",
    "\n",
    "# Esse comando cria o arquivo kaggle.json na instância no local correto: /home/<seu_usuario/.kaggle/kaggle.json\n",
    "!echo 'coloque o conteudo do kaggle.json aqui' >> ~/.kaggle/kaggle.json\n",
    "\n",
    "# Por questões de segurança, garanta que outros usuários do seu computador não tenham acesso de leitura \n",
    "# às suas credenciais. Esse comando é necessário, do contrário a API do Kaggle se recusa a fazer o download das bases de dados.\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com a chave de API do Kaggle, agora é a vez de instalar a API do kaggle\n",
    "!sudo pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Termos da competição\n",
    "\n",
    "Um passo importante, antes de baixar os dados do Kaggle, é aceitar os termos da competição. Para isso, acesse a seção `rules` na [página da competição](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/rules) e clique em `I Understand and Accept`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download do Kaggle\n",
    "\n",
    "Agora que temos a conta, chave de API e pacote instalado, basta realizarmos o download das imagens com as expressões faciais direto do kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comando para alterar pasta para '~/.fastai/data', onde os arquivos serão armazenados. \n",
    "%cd ~/.fastai/data/\n",
    "\n",
    "# Finnalmente, download do arquivo FER 2013 a partir da conta do Kaggle (fer2013.tar.gz)\n",
    "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use os comandos abaixo para criar uma pasta para FER (face expression recognition)\n",
    "# e para descompactar o arquivo dentro da pasta FER\n",
    "!mkdir FER\n",
    "!mv fer2013.tar.gz FER\n",
    "!rm example_submission.csv\n",
    "!tar -xvf FER/fer2013.tar.gz -C FER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listando arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se os arquivos foram decompactados corretamente\n",
    "%cd ~/.fastai/data/\n",
    "PATH = Path('./FER/fer2013')\n",
    "PATH.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nessa competição, as imagens foram armazenadas em forma de pixels, com os valores RGB de cada\n",
    "# A 1a coluna apresenta a emoção (expressão) a qual a imagem representa\n",
    "# A 2a coluna apresenta a imagem em si, num arranjo de números RFB que representa os pixels\n",
    "# A 3a coluna traz a utilidade da imagem, se será utilizada para treinamento, teste público ou teste privado (detalhes mais adiante).\n",
    "df = pd.read_csv(PATH/'fer2013.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As expressões das imagens na descrição do desafio do Kaggle, de 0 a 6 são\n",
    "# Raiva, Nojo, Medo, Alegria, Tristeza, Surpresa, Neutro\n",
    "labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "for l in range(len(labels)):\n",
    "    print(f'{l} = {labels[l]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Também temos a utilização das imagens, treinamento, teste público e teste privado \n",
    "# Os desafios Kaggle possuem um teste público, com a base que disponibilizam para criação dos modelos\n",
    "# e uma privada que somente é testada com a submissão dos resultados\n",
    "usages = df['Usage'].unique();usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos separar os data frames em treinamento, teste público e teste privado\n",
    "df_train = df[df['Usage'] == 'Training']\n",
    "df_valid = df[df['Usage'] == 'PublicTest']\n",
    "df_test = df[df['Usage'] == 'PrivateTest']\n",
    "\n",
    "# Os tamanhos de cada data frame\n",
    "n_train = len(df_train)\n",
    "n_valid = len(df_valid)\n",
    "n_test = len(df_test)\n",
    "n = len(df)\n",
    "\n",
    "print(f'{n_train} (Training) + {n_valid} (PublicTest) + {n_test} (PrivateTest) = {n} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograma das bases de treinamento, teste público e teste privado\n",
    "def setup_axe(axe,df,title):\n",
    "    axe.hist(df['emotion'])\n",
    "    axe.set_xticks(list(range(len(labels))))\n",
    "    axe.set_xticklabels(labels, rotation=45)\n",
    "    axe.set_title(title)\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "setup_axe(axes[0],df_train,'treinamento')\n",
    "setup_axe(axes[1],df_valid,'validação')\n",
    "setup_axe(axes[2],df_test,'teste')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos avaliar a proporção das imagens na base de treinamento de cada uma das 7 expressões\n",
    "ne = df_train['emotion'].value_counts(ascending=True)\n",
    "for k,v in zip(ne.keys(),ne.values):\n",
    "    pct = round(v/n*100,2)\n",
    "    print(f'({pct}%) {v} {labels[k]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função abaixo recria a imagem a partir dos pixels. Lembrando que as imagens são sequências de 3 números para cada pixel, ex (48,48,3)\n",
    "# Vamos representar a expressão como um inteiro e os pixels de uma imagem como um array numpy \n",
    "def row2image(row):\n",
    "    pixels, emotion = row['pixels'], row['emotion']\n",
    "    img = np.array(pixels.split())\n",
    "    img = img.reshape(48,48)\n",
    "    image = np.zeros((48,48,3))\n",
    "    image[:,:,0] = img\n",
    "    image[:,:,1] = img\n",
    "    image[:,:,2] = img\n",
    "    return np.array([image.astype(np.uint8), emotion])\n",
    "\n",
    "# Apresenta imagem já convertida com o título da expressão\n",
    "def show(img_title):\n",
    "    plt.imshow(img_title[0])\n",
    "    plt.title(labels[img_title[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos dar uma olhada na base para ver as imagens? Brinquem à vontade, aqui não somos Gilberto Gil, mas vamos de 2222\n",
    "row = df_train.iloc[2222]\n",
    "img = row2image(row)\n",
    "show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Armazenar os dados (pixels) em forma de imagens nas pastas train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com a função de pixels (com 3 números cada) para arquivos, vamos agora criar as pastas para armazenar cria as pastas train, valid e test\n",
    "Path(PATH/'train').mkdir(exist_ok=True)\n",
    "Path(PATH/'valid').mkdir(exist_ok=True)\n",
    "Path(PATH/'test').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define as variáveis das pastas de treinamento, teste público e teste privado\n",
    "PATH_train = PATH/'train'\n",
    "PATH_valid = PATH/'valid'\n",
    "PATH_test = PATH/'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria as 7 pastas das expressões em cada uma das pastas train, train, val e test\n",
    "for l in labels:\n",
    "    Path(PATH_train/l).mkdir(exist_ok=True)\n",
    "    Path(PATH_valid/l).mkdir(exist_ok=True)\n",
    "    Path(PATH_test/l).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria as imagens a partir dos pixels e salva nas subpastas correspondentes. Aqui utilizaremos o teste público como validação\n",
    "# e teste privado como teste\n",
    "def createImages(df,path):\n",
    "    for e in range(len(labels)):\n",
    "        df_e = df[df['emotion'] == e]\n",
    "        path_e = path/labels[e]\n",
    "        i=0\n",
    "        for index, row in df_e.iterrows():\n",
    "            img = row2image(row)\n",
    "            image = PIL.Image.fromarray(img[0], 'RGB')\n",
    "            fname = str(e)+'_'+str(i)+'.jpg'\n",
    "            image.save(path_e/fname)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula o tempo para criação das imagens e posicionamento nas subpastas\n",
    "%%time\n",
    "createImages(df_train,PATH_train)\n",
    "createImages(df_valid,PATH_valid)\n",
    "createImages(df_test,PATH_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos dar uma olhada se a criação ocorreu da forma desejada\n",
    "p = PATH_train/'Angry'\n",
    "p.ls()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criar o ImageDataBunch parametrizado para resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria transformações para as imagens como forma de aumentar o dataset  data augmentation\n",
    "tfms = get_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageDataBunch\n",
    "# bs=16, size=299 : ajustamos os parametros para o laboratório terminar num tempo razoável, mas o melhor seria o valor \n",
    "# do hiperparâmetro size=299 para ajudar à resnet50\n",
    "data = ImageDataBunch.from_folder(PATH, ds_tfms=tfms, bs=16, size=224)\n",
    "data.normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(rows=3, figsize=(7,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.classes, data.c, len(data.train_ds), len(data.valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinar o modelo com transfer learning (resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria o learner a partir do resnet50 com métrica de acurácia para buscar dos coeficientes da rede neural\n",
    "learn = cnn_learner(data, models.resnet50, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one cycle training com 4 épocas\n",
    "learn.fit_one_cycle(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando o estágio atual (caso o preemptive resolve nos ajudar O.o)\n",
    "learn.save('fer2013-stage-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fer2013-stage-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com o modelo treinado somente com as novas imagens, vamos \"descongelar\" o learner para reaprender com o resnet50 completo\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buscar os melhores learning rates\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# com base no gráfico vamos retreinar com 6 épocas e learning rates específicos\n",
    "learn.fit_one_cycle(6, max_lr=slice(1e-6,1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvar estágio atual\n",
    "learn.save('fer2013-stage-2')\n",
    "\n",
    "# exportar o modelo treinado. Importante para implantarmos no nosso aplicativo web!\n",
    "learn.export('modelo.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados\n",
    "\n",
    "Com o modelo treinado, vamos avaliar os resultados, principalmente com relação a acurácia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fer2013-stage-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A matriz de confusão mostra que, para as expressões com mais dados para treinamento temos um acerto maior\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando quais foram as piores classificações em termos de erro\n",
    "interp.plot_top_losses(9, figsize=(15,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_probs, val_targets = learn.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = torch.argmax(val_probs,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = (val_preds == val_targets).type(torch.FloatTensor).mean().item()\n",
    "print(f'valid accuracy: {round(val_acc*100,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apesar de parecer pouco, um modelo com 64% de acurácia estaria bem colocado no desafio do Kaggle, isso sem muito esforço, pois poderíamos melhorar o tamanho da imagem e realizar limpeza das imagens expúreas/dúbias.\n",
    "\n",
    "\n",
    "# Utilizando o modelo em produção (predição de novas fotos)\n",
    "\n",
    "Agora vamos testar nosso modelo com aquelas imagens que não foram utilizadas nem no treino nem na validação. Brinquem à vontade, alterando a pasta de 'Happy' para outras expressões, bem como o índice do array de arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image from test folder (images arquived in subfolders)\n",
    "p = PATH_test/'Happy'\n",
    "url = p.ls()[1]\n",
    "img = open_image(url)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction\n",
    "#probs = learn.predict(img)\n",
    "#prediction = learn.data.classes[probs.argmax()]\n",
    "#prediction\n",
    "\n",
    "pred_class,pred_idx,outputs = learn.predict(img)\n",
    "pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscando imagem da internet\n",
    "\n",
    "Vamos tentar buscar uma imagem na internet e realizar uma classificação da expressão facial? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download da imagem\n",
    "!wget http://thesharpe.com/wp-content/uploads/2017/01/surprise-someone-1-1024x768.jpg\n",
    "    \n",
    "# Transformação para grayscale (as imagens treinadas e validadas também estão em grayscale)\n",
    "img = PIL.Image.open('surprise-someone-1-1024x768.jpg').convert('LA')\n",
    "\n",
    "# Salvar a imagem (percebam que o formato deve ser png)\n",
    "img.save('surprise-someone-1-1024x768.png')    \n",
    "    \n",
    "# Mudança de semântica! Antes img era uma imagem do pacote PIL, agora é uma imagem fast.ai (open_image)    \n",
    "img = open_image('./surprise-someone-1-1024x768.png')\n",
    "\n",
    "# Vamos dar uma olhada na imagem baixada em grayscale\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos realizar as transformações utilizadas nas imagens de treino? A imagem a ser classificada será reduzida e poderá\n",
    "# sofrer algumas rotações e distorções\n",
    "tfms = get_transforms()\n",
    "\n",
    "for transformation in tfms:\n",
    "    img = img.apply_tfms(transformation, size=224)\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalmente, vamos prever o que o Kevin McCallister estava expressando\n",
    "pred_class,pred_idx,outputs = learn.predict(img)\n",
    "pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo treinado numa aplicação web\n",
    "\n",
    "Agora que temos um modelo treinado, funcionando razoavelmente bem, que tal criarmos uma aplicação web que recebe fotos e classifica a expressão do rosto?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
